#####################################################################################
# File Name: static_rerank_eval.py
#
# Description:
# This module implements a evaluation framework for static re-rankers
# in recommender systems. Static re-rankers are post-processing methods applied to
# the output of recommendation models to refine the ranked lists of items presented
# to users. The evaluation framework supports ranking and diversity metrics,
# and caching mechanisms for efficient processing.
# The features of this module include:
#
# 1. Caching Mechanisms:
#    - Avoid redundant computations by caching ranked items and scores for users.
#
# 2. Evaluation Metrics:
#    - Supports ranking metrics and diversity metrics.
#
#
# 3. Re-Ranking Framework:
#    - Allows evaluation of multiple re-rankers, each modifying the ranked lists
#      generated by the base recommendation model.
#
# 4. Results Handling:
#    - Aggregates results across users and metrics.
#    - Stores results in structured formats (e.g., JSON) for easy analysis and debugging.
#
#####################################################################################


from collections import OrderedDict
import time

import numpy as np

from tqdm.auto import tqdm
from ..rerankers import ReRanker

from ..experiment.result import Result
import json

import pandas as pd
import random


def cache_rankings(model, user_idx, item_indices, k = -1):
    """Caches ranked items and scores for a user to avoid redundant computations.
    Parameters
    ----------
    model : object
        The recommendation model.
    user_idx : int
        User index.
    item_indices : list or None
        Indices of items to rank.
    k (int, required) â€“ Cut-off length for recommendations, k=-1 will return ranked list of all items. This is more important for ANN to know the limit to avoid exhaustive ranking.

    Returns
    -------
    tuple
        Ranked items and scores for the user.
    """
    # with lock:  # Locking the cache access to avoid race conditions
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}
    if not hasattr(model, 'item_scores_mapped_indices'):
        model.item_scores_mapped_indices  = {}

    if user_idx in model.ranked_items and user_idx in model.item_scores:
        return model.ranked_items[user_idx], model.item_scores[user_idx]


    # item_idx2id = {v: k for k, v in test_set.iid_map.items()} # cornac item ID : raw item ID
    # user_idx2id = {v: k for k, v in test_set.uid_map.items()} # cornac user ID : raw user ID
    # item_id2idx = {k: v for k, v in test_set.iid_map.items()} # raw item ID : cornac item ID
    if not getattr(model, 'is_fitted', False):
        raise RuntimeError("Model is not fitted. Re-ranking requires the model to be fitted or the candidate lists for all users to be ready. Please call `model.fit()` before ranking.")


    item_rank, item_scores = model.rank( user_idx=user_idx, item_indices=item_indices, k=k) 


    # Cache the results for future use
    model.ranked_items[user_idx] = item_rank
    model.item_scores[user_idx] = item_scores
    

    return item_rank, item_scores




def cache_rerankings(reranker, user_idx, train_set, model_ranked_items, model_ranked_scores):
    '''
    Helper function to compute or retrieve cached re-ranked items for a specific user.

    This function checks if re-ranked items for a user are already cached in the `reranker` object.
    If cached, it retrieves and returns the re-ranked items. If not, it computes the re-ranking
    using the provided `reranker` object, caches the result, and then returns it.

    Parameters:
    - `reranker`: The re-ranking object responsible for performing the re-ranking process.
    - `user_idx`: The index of the user for whom re-ranking is performed.
    - `train_set`: cornac.data.Dataset. User-Item preference data used to inform the re-ranking process.
    -  `test_set`: cornac.data.Dataset.
    - `model_ranked_items`: A list of items pre-ranked by the recommendation model.
    - `model_ranked_scores`: Scores generated by the initial recommender model. This parameter can 
      be set to `None` as the current re-rankers do not utilize these scores.


    Workflow:
    1. Checks for cached re-ranked items in `reranker.ranked_items`. If available, returns them.
    2. If not cached:
    - Computes the re-ranked items using the `reranker.rerank` method.
    - Caches the re-ranked items in a thread-safe manner using `lock`.
    -  Measures the time taken for the re-ranking process.

    Returns:
    - `reranked_items`: The list of re-ranked items for the specified user.
    '''

    if not isinstance(reranker, ReRanker):
        raise ValueError(
            f"Reranker {reranker} is not an instance of `cornac.rerankers.ReRanker`.")
    
    if hasattr(reranker, 'ranked_items') and len(reranker.ranked_items) > 0 and user_idx in reranker.ranked_items:
        return reranker.ranked_items[user_idx]
    
    if not hasattr(reranker, 'ranked_items'):
        reranker.ranked_items = {}
    
    # item_idx2id = {v: k for k, v in test_set.iid_map.items()} # cornac item ID : raw item ID
    # user_idx2id = {v: k for k, v in test_set.uid_map.items()} # cornac user ID : raw user ID
    # item_id2idx = {k: v for k, v in test_set.iid_map.items()} # raw item ID : cornac item ID


    # item_rank, item_scores = model.rank( user_idx=user_idx, item_indices=item_indices, k=k,item_idx2id = item_idx2id, user_idx2id = user_idx2id, item_id2idx =  item_id2idx) 

    start_time = time.time()
    # print(f"Reranker:{reranker.name}, model rec, score len:{len(model_ranked_items)},{len(model_ranked_scores)}")
    reranked_items = reranker.rerank(
        user_idx=user_idx, interaction_history=train_set, candidate_items=model_ranked_items,
        prediction_scores=model_ranked_scores
        # item_idx2id = item_idx2id, user_idx2id = user_idx2id, item_id2idx =  item_id2idx
    )
    reranking_time = time.time() - start_time

    reranker.ranked_items[user_idx] = reranked_items

    if not hasattr(reranker, 'cumulative_time'):
        reranker.cumulative_time = 0
        reranker.user_count = 0
    reranker.cumulative_time += reranking_time
    reranker.user_count += 1

    return reranked_items

def ranking_eval_on_rerankers(
        model,
        metrics,
        rerankers,
        train_set,
        test_set,
        val_set=None,
        rating_threshold=1.0,
        exclude_unknowns=True,
        verbose=False
):
    '''
    Evaluation of ranking metrics for multiple rerankers.

    Parameters: 
    - `model`: The recommender model used to generate initial ranked lists.
    - `metrics`: A list of ranking metrics to compute (e.g., precision, recall).
    - `rerankers`: A list of rerankers to apply on the model's output.
    - `train_set`: `cornac.data.Dataset`. The training dataset containing user-item interactions.
    - `test_set`: `cornac.data.Dataset`. The test dataset containing user-item interactions.
    - `val_set` (optional): `cornac.data.Dataset`. The validation dataset containing user-item interactions.
    - `rating_threshold`: The minimum rating required for an item to be considered positive.
    - `exclude_unknowns`: Boolean. Whether to exclude items not seen in the training set.
    - `verbose`: Boolean. Whether to display progress.


    Returns:
    - `reranked_avg_results`: A list containing the averaged evaluation scores for each reranker.
    - `reranked_results_per_method`: A nested list of dictionaries storing per-user scores for 
    each reranker and metric.

    Notes:
    - The function is thread-safe and ensures shared data structures are accessed securely.
    - Results and metadata are saved to the disk for further analysis.

    '''
    if len(metrics) == 0:
        return [], []
    
    max_k = max(m.k for m in metrics)

    reranked_results_per_method = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]

    reranked_avg_results = [
        [0 for _ in enumerate(metrics)] for _ in range(len(rerankers))]

    # Initialize cache if not already present
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}
    
    test_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix

    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]
    test_user_indices = set(test_set.uir_tuple[0])

    # Assuming test_user_indices is a set
    # random_users = random.sample(test_user_indices, min(5, len(test_user_indices)))

    # print(f"Randomly selected {len(random_users)} users:", random_users)
    for user_idx in tqdm(
        test_user_indices, desc="Ranking Metrics Eval on Re-ranking Results", disable=not verbose, miniters=100
    ):
    # for user_idx in tqdm(
    #     random_users, desc="Ranking Metrics Eval on Re-ranking Results", disable=not verbose, miniters=100
    # ):
        test_pos_items = pos_items(test_mat.getrow(user_idx))
        if len(test_pos_items) == 0:
            continue
        # binary mask for ground-truth positive items
        u_gt_pos_mask = np.zeros(test_set.num_items, dtype="int")
        u_gt_pos_mask[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(val_mat.getrow(user_idx))
        train_pos_items = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )
        # binary mask for ground-truth negative items, removing all positive items
        u_gt_neg_mask = np.ones(test_set.num_items, dtype="int")
        u_gt_neg_mask[test_pos_items + val_pos_items + train_pos_items] = 0
         # filter items being considered for evaluation
        if exclude_unknowns:
            u_gt_pos_mask = u_gt_pos_mask[: train_set.num_items]
            u_gt_neg_mask = u_gt_neg_mask[: train_set.num_items]
        item_indices = np.nonzero(u_gt_pos_mask + u_gt_neg_mask)[0]
        u_gt_pos_items = np.nonzero(u_gt_pos_mask)[0]
        u_gt_neg_items = np.nonzero(u_gt_neg_mask)[0]
        # item_rank, item_scores = cache_rankings(
        # model, user_idx,  item_indices, k = -1)
        item_rank, item_scores = cache_rankings(
        model,   user_idx=user_idx, item_indices=item_indices, k=-1)

        item_scores_mapped_indices = getattr(model, "item_scores_mapped_indices", {})
        if item_scores_mapped_indices is None or len(item_scores_mapped_indices) == 0:
            print(f"Warning: Item score mapped indices of the model {model.name} is empty. Proceeding with default behavior: assuming item scores correspond to all available item indices in order [0, 1, ..., N].")
            impression_items_list =  list(np.arange(test_set.num_items))
        else:
            # Check if the user exists in the mapping
            if user_idx not in item_scores_mapped_indices:
                raise ValueError(f"user_idx {user_idx} not found in model.item_scores_mapped_indices.")
            impression_items_list = list(item_scores_mapped_indices[user_idx])
        
        
        if len(impression_items_list) < len(item_scores):
            raise ValueError(
                f"Mismatch: the number of item indices ({len(impression_items_list)}) is smaller than the number of item scores ({len(item_scores)}). "
                f"Ensure that the item scores are correctly mapped to the intended item indices. "
                f"Check the model's rank() function to verify that both the scores and corresponding item indices are properly saved during the ranking stage."
            )


        item_idx_to_score = {item: score for item,
                        score in zip(impression_items_list, item_scores)}
        
        missing_item_index = [item for item in item_rank if item not in item_idx_to_score]
        if missing_item_index:
            raise KeyError(
                f"The following items in the ranked list are missing from the score mapping: {missing_item_index[:10]} "
                f"{'(truncated)' if len(missing_item_index) > 10 else ''}. "
                f"Ensure that all items in the ranked list have corresponding scores in the score mapping. "
                f"If the issue persists, check for inconsistencies between the ranked list, the available item scores and item indices."
            )

        
        # Retrieve the scores from recommender models corresponding to ranked_items
        ranked_scores = [item_idx_to_score[item] for item in item_rank]
        ranked_items = list(item_rank)

        user_results = [
            {} for _ in range(len(rerankers))]

        for j in range(len(rerankers)):
            reranker = rerankers[j]
            reranked_items = cache_rerankings(
                reranker, user_idx, train_set,  ranked_items, ranked_scores)
    
            for i, mt in enumerate(metrics):
                mt_score = mt.compute(
                    gt_pos=u_gt_pos_items,
                    gt_neg=u_gt_neg_items,
                    pd_rank=reranked_items,
                    pd_scores=item_scores,
                    item_indices=item_indices,
                )
                user_results[j][i] = mt_score
                reranked_results_per_method[j][i][user_idx] = mt_score

    for reranker_id in range(len(rerankers)):
        result = []
        for i, mt in enumerate(metrics):
            user_values = reranked_results_per_method[reranker_id][i]
            num_users = len(user_values)
            result = sum(user_values.values()) / \
                len(user_values) if num_users > 0 else -1
            reranked_avg_results[reranker_id][i] = result
        reranker = rerankers[reranker_id]
        
    # After evaluating recomemndations for all users, calculate average computation time for each reranker
    avg_reranking_times = {}
    for reranker in rerankers:
        if hasattr(reranker, 'cumulative_time') and reranker.user_count > 0:
            avg_time = reranker.cumulative_time / reranker.user_count
            avg_reranking_times[reranker.name] = avg_time
            print(
                f"Reranker: {reranker.name}, Avg. Reranking Time: {avg_time:.4f} seconds")
        else:
            avg_reranking_times[reranker.name] = None

    return reranked_avg_results, reranked_results_per_method



def preprocess_data_for_Fragmentation(user_idx, test_set,  train_set, model, reranker, metrics, item_indices):
    '''
    Preprocess data for evaluating the "Fragmentation" metric.

    This function prepares sampled ranked lists of items for other users to evaluate 
    metrics involving fragmentation. It ensures the sampled data excludes the current user 
    and adheres to the constraints defined by the metric, such as the number of samples (`n_samples`) 
    and the top-k items (`k`).

    Parameters:
    - `user_idx`: The index of the current user being evaluated.
    - `test_set`: The test dataset containing user-item interactions.
    - `train_set`: The training dataset containing user-item interactions.
    - `model`: The recommender model used to generate ranked lists.
    - `reranker`: The re-ranking object responsible for adjusting the ranked lists.
    - `metrics`: A list of evaluation metrics, including the "Fragmentation" metric.
    - `item_indices`: A list of item indices to consider during ranking and re-ranking.

    Returns:
    - `pd_other_users`: A list of lists where each sub-list corresponds to a metric. 
        For metrics other than 'Fragmentation,' an empty list is used.

    '''
    pd_other_users = [] 
    max_k = max(m.k for m in metrics)

    for i, mt in enumerate(metrics):
        if "Fragmentation" in mt.name:
            if len(model.ranked_items) > mt.n_samples:
                other_users = [key for key,
                               value in model.ranked_items.items()]

                # Exclude the current user (user_idx) from the candidate list
                other_users.remove(user_idx)

            else:
                test_user_indices = set(test_set.uir_tuple[0])
                other_users = list(test_user_indices)
                other_users.remove(user_idx)  # Exclude the current user

            # Sample from other users
            sampled_users = np.random.choice(
                other_users, size=mt.n_samples, replace=False)

            # Process samples based on the value of mt.k
            sample_rank = []

            # Separate cached and uncached samples
            for x in sampled_users:
                # model_ranked_items, model_ranked_scores = cache_rankings(
                #     model, x, item_indices, k = -1)
                model_ranked_items, model_ranked_scores = cache_rankings(
        model,   user_idx = x, item_indices=item_indices, k=-1)
                
                ## To-do: need to decide the input tp cache_rerankings
                reranked_items_x = cache_rerankings(
                    reranker, x, train_set,  model_ranked_items, model_ranked_scores
                )
                if len(reranked_items_x) >= mt.k and mt.k > 0:
                    sample_rank.append(reranked_items_x[:mt.k])
                else:
                    sample_rank.append(reranked_items_x)

            pd_other_users.append(sample_rank)
        else:
            pd_other_users.append([])
    return pd_other_users


def diversity_eval_on_rerankers(
        model,
        metrics,
        rerankers,
        train_set,
        test_set,
        val_set=None,
        rating_threshold=1.0,
        exclude_unknowns=True,
        verbose=False
):
    '''
    Evaluation of diversity metrics for multiple rerankers.

    This function evaluates the diversity performance of multiple rerankers, 
    calculating metrics for each user and reranker, and aggregating the results. 

    Parameters:
    - `model`: The recommendation model used for generating initial rankings.
    - `metrics`: A list of diversity metrics to compute.
    - `rerankers`: A list of rerankers to evaluate.
    - `train_set`: The training dataset containing user-item interactions.
    - `test_set`: The test dataset containing user-item interactions.
    - `val_set` (optional): The validation dataset containing user-item interactions.
    - `rating_threshold`: Minimum rating required for an item to be considered positive.
    - `exclude_unknowns`: Whether to exclude unknown items during evaluation.
    - `verbose`: Boolean. Whether to display progress and log details.



    Returns:
    - `reranked_avg_results`: A list of averaged diversity metric scores for each reranker.
    - `reranked_results_per_method`: A nested list of dictionaries containing per-user metric scores for each reranker.

    '''
    if len(metrics) == 0:
        return [], []

    max_k = max(m.k for m in metrics)

    reranked_results_per_method = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]
    reranked_avg_results = [
        [0 for _ in enumerate(metrics)] for _ in range(len(rerankers))]



    # Initialize cache if not already present
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}
    
    test_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix


    user_history_dict = OrderedDict()

    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]
    test_user_indices = set(test_set.uir_tuple[0])
    for user_idx in test_user_indices:
        pos_item_idx = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )
        user_history_dict[user_idx] = pos_item_idx
    
    globalProbs = []

    for i, mt in enumerate(metrics):
        if "Binomial" in mt.name:
            global_prob = mt.globalFeatureProbs(user_history_dict)
            globalProbs.append(global_prob)
        else:
            globalProbs.append([])


    for user_idx in tqdm(
        test_user_indices, desc="Diversity Eval on Re-ranking Results", disable=not verbose, miniters=100
    ):
        test_pos_items = pos_items(test_mat.getrow(user_idx))
        if len(test_pos_items) == 0:
            continue
        # binary mask for ground-truth positive items
        u_gt_pos_mask = np.zeros(test_set.num_items, dtype="int")
        u_gt_pos_mask[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(val_mat.getrow(user_idx))
        train_pos_items = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )
        # binary mask for ground-truth negative items, removing all positive items
        u_gt_neg_mask = np.ones(test_set.num_items, dtype="int")
        u_gt_neg_mask[test_pos_items + val_pos_items + train_pos_items] = 0

        # filter items being considered for evaluation
        if exclude_unknowns:
            u_gt_pos_mask = u_gt_pos_mask[: train_set.num_items]
            u_gt_neg_mask = u_gt_neg_mask[: train_set.num_items]
        
        item_indices = np.nonzero(u_gt_pos_mask + u_gt_neg_mask)[0]
        # item_rank, item_scores = cache_rankings(
        # model, user_idx, item_indices, k = -1)

        item_rank, item_scores = cache_rankings(
        model,   user_idx=user_idx, item_indices=item_indices,  k=-1)

        item_scores_mapped_indices = getattr(model, "item_scores_mapped_indices", {})
        if item_scores_mapped_indices is None or len(item_scores_mapped_indices) == 0:
            print(f"Warning: Item score mapped indices of the model {model.name} is empty. Proceeding with default behavior: assuming item scores correspond to all available item indices in order [0, 1, ..., N].")
            impression_items_list =  list(np.arange(test_set.num_items))
        else:
            # Check if the user exists in the mapping
            if user_idx not in item_scores_mapped_indices:
                raise ValueError(f"user_idx {user_idx} not found in model.item_scores_mapped_indices.")
            impression_items_list = list(item_scores_mapped_indices[user_idx])
        
        
        if len(impression_items_list) < len(item_scores):
            raise ValueError(
                f"Mismatch: the number of item indices ({len(impression_items_list)}) is smaller than the number of item scores ({len(item_scores)}). "
                f"Ensure that the item scores are correctly mapped to the intended item indices. "
                f"Check the model's rank() function to verify that both the scores and corresponding item indices are properly saved during the ranking stage."
            )


        item_idx_to_score = {item: score for item,
                        score in zip(impression_items_list, item_scores)}
        
        missing_item_index = [item for item in item_rank if item not in item_idx_to_score]
        if missing_item_index:
            raise KeyError(
                f"The following items in the ranked list are missing from the score mapping: {missing_item_index[:10]} "
                f"{'(truncated)' if len(missing_item_index) > 10 else ''}. "
                f"Ensure that all items in the ranked list have corresponding scores in the score mapping. "
                f"If the issue persists, check for inconsistencies between the ranked list, the available item scores and item indices."
            )

        
        # Retrieve the scores from recommender models corresponding to ranked_items
        ranked_scores = [item_idx_to_score[item] for item in item_rank]
        ranked_items = list(item_rank)
        
        # item_rank, item_scores = cache_rankings(
        # model,   user_idx=user_idx, item_indices=item_indices,test_set = test_set,  k=-1)

        # item_idx_to_score = {item: score for item,
        #                  score in zip(item_indices, item_scores)}
        # # Retrieve the scores corresponding to ranked_items
        # ranked_scores = [item_idx_to_score[item] for item in item_rank]
        # ranked_items = list(item_rank)

        # if rerankers_item_pool is None or not isinstance(rerankers_item_pool, (list, np.ndarray)) or len(rerankers_item_pool) == 0:
        pool_ids = np.arange(test_set.num_items)
        # else:
        #     pool_ids = np.asarray(rerankers_item_pool)
        u_gt_rating = np.zeros(test_set.num_items)
        # Get non-zero values and column indices for the user's row
        gd_item_idx = test_mat.getrow(user_idx).indices
        gd_item_rating = test_mat.getrow(user_idx).data
        u_gt_rating[gd_item_idx] = gd_item_rating

        user_history = user_history_dict.get(user_idx, [])
  
        pd_other_users = []
        
  
        # Compute metric times and store results
        user_results = [
            {} for _ in range(len(rerankers))]
        for j in range(len(rerankers)):
            reranker = rerankers[j]
            if not isinstance(reranker, ReRanker):
                raise ValueError(
                    f"Reranker {reranker} is not an instance of `cornac.rerankers.ReRanker`.")
            reranked_items = cache_rerankings(
                reranker, user_idx, train_set, ranked_items, ranked_scores)
            pd_other_users = preprocess_data_for_Fragmentation(
                user_idx, test_set,  train_set, model, reranker, metrics, item_indices)
            for i, mt in enumerate(metrics):
                mt_score = mt.compute(
                    pd_rank=reranked_items,
                    pd_scores=item_scores,
                    rating_threshold=rating_threshold,
                    gt_ratings=u_gt_rating,
                    globalProb=globalProbs[i],
                    user_history=user_history,
                    pool=pool_ids,
                    pd_other_users=pd_other_users[i]
                )
                if mt_score is None:
                    pass
                else:
                    user_results[j][i] = mt_score
                    reranked_results_per_method[j][i][user_idx] = mt_score

    # Average results

    for reranker_id in range(len(rerankers)):
        result = []
        for i, mt in enumerate(metrics):
            user_values = reranked_results_per_method[reranker_id][i]
            num_users = len(user_values)
            result = sum(user_values.values()) / \
                len(user_values) if num_users > 0 else -1
            reranked_avg_results[reranker_id][i] = result

    return reranked_avg_results, reranked_results_per_method


class StaticReRankEval:
    """Evaluate static re-rankers.

    """

    def __init__(
            self, BaseEvaluator):
        """
        Parameters
            ----------
            BaseEvaluator: Base evaluator object with the necessary datasets and configuration.
              "cornac.eval_methods.RatioSplit" object. 
        """
        self.BaseEvaluator = BaseEvaluator

    def _eval(self, model, test_set, val_set,  rerankers,  rating_metrics,
        ranking_metrics,
        diversity_metrics):
        """
        Internal method to perform evaluation for ranking and diversity metrics.

        Parameters
        ----------
            model : cornac.models.Recommender
                A recommender model whose output will be evaluated.
                The model is expected to generate ranked lists of items for each user 
                and support the `rank` method.

            test_set : cornac.data.Dataset
                The test dataset containing user-item interactions.
                This dataset is used to evaluate the recommendation and re-ranking 
                performance of the model and re-rankers.

            val_set : cornac.data.Dataset or None
                The validation dataset, which may contain additional user-item interactions.
                If None, the validation step is skipped.

            rerankers : list of cornac.rerankers.ReRanker
                A list of static re-rankers to apply to the ranked lists generated by the model.
                Each re-ranker modifies the rankings to achieve specific goals, such as 
                improving diversity.


            Returns
            -------
            Result : cornac.experiment.Result
                An object containing the aggregated metric results and per-user results for each 
                metric and re-ranker.


            Notes
            -----
            - This method integrates the evaluation of both ranking and diversity metrics. 
        """
        metric_avg_results = OrderedDict()
        metric_user_results = OrderedDict()

        reranked_avg_results, reranked_results_per_method = ranking_eval_on_rerankers(
            model=model,
            metrics= ranking_metrics,
            rerankers=rerankers,
            train_set=self.BaseEvaluator.train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=self.BaseEvaluator.rating_threshold,
            exclude_unknowns=self.BaseEvaluator.exclude_unknowns,
            verbose=self.BaseEvaluator.verbose
        )
        for j in range(len(rerankers)):
            for i, mt in enumerate(ranking_metrics):
                new_name = rerankers[j].name + "_"+mt.name
                metric_avg_results[new_name] = reranked_avg_results[j][i]
                metric_user_results[new_name] = reranked_results_per_method[j][i]

        reranked_avg_results, reranked_results_per_method = diversity_eval_on_rerankers(
            model=model,
            metrics= diversity_metrics,
            rerankers=rerankers,
            train_set=self.BaseEvaluator.train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=self.BaseEvaluator.rating_threshold,
            exclude_unknowns=self.BaseEvaluator.exclude_unknowns,
            verbose=self.BaseEvaluator.verbose
        )

        for j in range(len(rerankers)):
            for i, mt in enumerate(diversity_metrics):
                new_name = rerankers[j].name + "_"+mt.name
                metric_avg_results[new_name] = reranked_avg_results[j][i]
                metric_user_results[new_name] = reranked_results_per_method[j][i]
        return Result(model.name, metric_avg_results, metric_user_results)

    def evaluate(self, model, metrics, user_based, rerankers, show_validation=True):
        """ Evaluate the static re-rankers.

        Parameters
        ----------
        model:`cornac.models.Recommender`
            Recommender model to be evaluated.  The model should generate ranked lists 
        of items for each user through its `rank` method.

        metrics:  A list of metrics (ranking and/or diversity metrics) to evaluate the re-rankers.


        user_based: bool
            Determines the evaluation strategy for rating metrics. Specifies whether results should 
        be averaged based on the number of users or the number of ratings. 
        **Note:** Currently, this parameter is not utilized because rating metrics are not 
        evaluated due to the lack of score predictions from re-ranking methods. 
        It is included for potential future support for rating metrics evaluation.

        rerankers : list of cornac.rerankers.ReRanker
            A list of static re-rankers to apply to the ranked lists generated by the model. These 
            re-rankers adjust the rankings to optimize specific objectives such as diversity.

        show_validation: bool, optional, default: True
            Whether to show the results on validation set (if exists).



        Returns
        -------
            test_result : cornac.experiment.Result
                The evaluation result on the test set. Contains:
                - Aggregated results for all metrics.
                - Per-user results for each metric and re-ranker.

            val_result : cornac.experiment.Result or None
                The evaluation result on the validation set, if `show_validation` is True and a validation 
                set is provided. Returns None if validation is not performed.

        Raises
        ------
        ValueError
            If any of the following conditions are met:
            - The training set (`train_set`) in the BaseEvaluator is not provided.
            - The test set (`test_set`) in the BaseEvaluator is not provided.
            - The list of re-rankers is not provided.
        """
        if self.BaseEvaluator.train_set is None:
            raise ValueError("train_set is required but None!")
        if self.BaseEvaluator.test_set is None:
            raise ValueError("test_set is required but None!")
        if rerankers is None:
            raise ValueError("rerankers is required but None!")

        rating_metrics, ranking_metrics,diversity_metrics = self.BaseEvaluator.organize_metrics(metrics)

        ##############
        # EVALUATION #
        ##############
        if self.BaseEvaluator.verbose:
            print("\n[{}] Static Re-ranking Evaluation started!".format(model.name))

        
        

        start = time.time()
        test_result = self._eval(
            model=model,
            test_set=self.BaseEvaluator.test_set,
            val_set=self.BaseEvaluator.val_set,
            rerankers=rerankers,
            rating_metrics = rating_metrics ,
        ranking_metrics = ranking_metrics,
        diversity_metrics = diversity_metrics
        )
        test_time = time.time() - start
        test_result.metric_avg_results["Static Re-Rank Time(s)"] = test_time
        # save_path = model.name+"_trained/"+"static_rerankers_exp"
        # test_result.save(save_path)

        val_result = None
        if show_validation and self.BaseEvaluator.val_set is not None:
            start = time.time()
            val_result = self._eval(
                model=model, test_set=self.BaseEvaluator.val_set, val_set=None, rerankers=rerankers,   rating_metrics = rating_metrics ,
        ranking_metrics = ranking_metrics,
        diversity_metrics = diversity_metrics
            )
            val_time = time.time() - start
            val_result.metric_avg_results["Static Re-Rank Time(s)"] = val_time

        return test_result, val_result
