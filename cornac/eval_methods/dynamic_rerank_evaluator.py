from collections import OrderedDict
import time

import numpy as np
from tqdm.auto import tqdm

import json
import os
from ..experiment.result import Result

from cornac.rerankers import DynamicReRanker
from ..metrics import MAP, AUC
import random

def cache_rankings(model, user_idx, item_indices,  k = -1):
    """Caches ranked items and scores for a user to avoid redundant computations.
    Parameters
    ----------
    model : object
        The recommendation model.
    user_idx : int
        User index.
    item_indices : list or None
        Indices of items to rank.
    k (int, required) â€“ Cut-off length for recommendations, k=-1 will return ranked list of all items. This is more important for ANN to know the limit to avoid exhaustive ranking.

    Returns
    -------
    tuple
        Ranked items and scores for the user.
    """
    # with lock:  # Locking the cache access to avoid race conditions
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}

    if user_idx in model.ranked_items and user_idx in model.item_scores:
        return model.ranked_items[user_idx], model.item_scores[user_idx]



    if not getattr(model, 'is_fitted', False):
        raise RuntimeError("Model is not fitted. Re-ranking requires the model to be fitted or the candidate lists for all users to be ready. Please call `model.fit()` before ranking.")


    # item_rank, item_scores = model.rank( user_idx=user_idx, item_indices=item_indices, k=k,item_idx2id = item_idx2id, user_idx2id = user_idx2id, item_id2idx =  item_id2idx) 
    item_rank, item_scores = model.rank( user_idx=user_idx, item_indices=item_indices, k=k)

    # Cache the results for future use
    model.ranked_items[user_idx] = item_rank
    model.item_scores[user_idx] = item_scores
    

    return item_rank, item_scores










def cache_dynamic_rerankings(reranker, user_idx, train_set, initial_item_rank, recommendation_list, prediction_scores):
    '''
    Helper function to compute or retrieve cached re-ranked items for a specific user.

    This function checks if re-ranked items for a user are already cached in the `reranker` object.
    If cached, it retrieves and returns the re-ranked items. If not, it computes the re-ranking
    using the provided `reranker` object, caches the result, and then returns it.

    Parameters:
    - `reranker`: The re-ranking object responsible for performing the re-ranking process.
    - `user_idx`: The index of the user for whom re-ranking is performed.
    - `train_set`: cornac.data.Dataset. User-Item preference data used to inform the re-ranking process.
    - `model_ranked_items`: A list of items pre-ranked by the recommendation model.
    - `model_ranked_scores`: Scores generated by the initial recommender model. This parameter can 
      be set to `None` as the current re-rankers do not utilize these scores.


    Workflow:
    1. Checks for cached re-ranked items in `reranker.ranked_items`. If available, returns them.
    2. If not cached:
    - Computes the re-ranked items using the `reranker.rerank` method.
    - Caches the re-ranked items in a thread-safe manner using `lock`.
    -  Measures the time taken for the re-ranking process.

    Returns:
    - `reranked_list`: The list of re-ranked items for the specified user in several iterations.
    '''
    if not isinstance(reranker, DynamicReRanker):
        raise TypeError(
            f"Reranker must be an instance of DynamicReRanker, but got {type(reranker)}.")

    if hasattr(reranker, 'ranked_items') and len(reranker.ranked_items) > 0 and user_idx in reranker.ranked_items:
        return reranker.ranked_items[user_idx]

    if not hasattr(reranker, 'ranked_items'):
        reranker.ranked_items = {}
    


    start_time = time.time()
    reranked_list = reranker.rerank(
                    user_idx = user_idx, interaction_history = train_set, candidate_items = initial_item_rank, prediction_scores = prediction_scores, recommendation_list = recommendation_list)


    reranking_time = time.time() - start_time

    reranker.ranked_items[user_idx] = reranked_list

    if not hasattr(reranker, 'cumulative_time'):
        reranker.cumulative_time = 0
        reranker.user_count = 0
    reranker.cumulative_time += reranking_time
    reranker.user_count += 1
    return reranked_list


def ranking_eval_on_dyn_rerankers(
        model,
        metrics,
        rerankers,
        train_set,
        test_set,
        val_set=None,
        rating_threshold=1.0,
        exclude_unknowns=True,
        verbose=False):
    """Evaluate recommendation generated by dynamic re-ranking on provided ranking metrics.

    Parameters
    ----------
    model: :obj:`cornac.models.Recommender`, required
        Recommender model to be evaluated.

    metrics: obj:`iterable`, required
        List of ranking metrics: `cornac.metrics.RankingMetric`.

    train_set : cornac.data.Dataset
        Training dataset used to exclude training interactions from evaluation.

    test_set : cornac.data.Dataset
        Test dataset for evaluation.

    rerankers: A list of dynamic rerankers to apply on the model's output.

    val_set: obj:`cornac.data.Dataset`, optional, default: None
        Validation dataset to be used for parameter selection. 

    exclude_unknowns: bool, optional, default: True
        Ignore unknown users and items during evaluation.

    rating_threshold: float, optional, default: 1.0
        The threshold to convert ratings into positive or negative feedback.

    verbose: bool, optional, default: False
        Output evaluation progress.


    Returns
    -------
    avg_results : list
        Average metric scores across all users for each metric.

    user_results : list of dict
        Per-user metric scores for each metric, with results grouped by users.

    """

    if len(metrics) == 0:
        return [], []

    max_k = max(m.k for m in metrics)
    reranked_results_per_method = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]
    reranked_results_avg_per_user = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]

    reranked_avg_results = [
        [0 for _ in enumerate(metrics)] for _ in range(len(rerankers))]
    
        # Initialize cache if not already present
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}
    
    test_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix
    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]
    
  
    test_user_indices = set(test_set.uir_tuple[0])
    for user_idx in tqdm(
            test_user_indices, desc="Ranking evaluation on dynamic rerankers", disable=not verbose, miniters=100
    ):
        test_pos_items = pos_items(test_mat.getrow(user_idx))
        if len(test_pos_items) == 0:
            continue

        # binary mask for ground-truth positive items
        u_gt_pos_mask = np.zeros(test_set.num_items, dtype="int")
        u_gt_pos_mask[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(
            val_mat.getrow(user_idx))
        train_pos_items = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )

        u_gt_neg_mask = np.ones(test_set.num_items, dtype="int")
        u_gt_neg_mask[test_pos_items + val_pos_items + train_pos_items] = 0

        # filter items being considered for evaluation
        if exclude_unknowns:
            u_gt_pos_mask = u_gt_pos_mask[: train_set.num_items]
            u_gt_neg_mask = u_gt_neg_mask[: train_set.num_items]

        item_indices = np.nonzero(u_gt_pos_mask + u_gt_neg_mask)[0]
        u_gt_pos_items = np.nonzero(u_gt_pos_mask)[0]
        u_gt_neg_items = np.nonzero(u_gt_neg_mask)[0]
   
        if len(u_gt_pos_items) == 0:
            continue  # Skip if no impression items are clicked for this user
        

        item_rank, item_scores = cache_rankings(
        model,   user_idx=user_idx, item_indices=item_indices, k=-1)

        ## currently dynamic re-ranking doesn't require item_score as input.
        # item_idx_to_score = {item: score for item,
        #                  score in zip(item_indices, item_scores)}
        # Retrieve the scores from recommender models corresponding to ranked_items
        # ranked_scores = [item_idx_to_score[item] for item in item_rank]
        ranked_items = list(item_rank)

            
        avg_results = [{} for _ in enumerate(metrics)]
        user_results = [{} for _ in enumerate(metrics)]

        for j in range(len(rerankers)):
            reranker = rerankers[j]
            result_all_iterations  = cache_dynamic_rerankings(reranker, user_idx, train_set, initial_item_rank = ranked_items, recommendation_list = ranked_items, prediction_scores= None) 
            if len(result_all_iterations) == 0:
                continue

            # cannot use ranking metrics that use pd_scores as inputs
            for i, mt in enumerate(metrics):
                if not isinstance(mt, MAP) and not isinstance(mt, AUC):
                    iteration_results = {}

                    for iteration, item_rank in result_all_iterations.items():
                        mt_score = mt.compute(

                            gt_pos=u_gt_pos_items,
                            gt_neg=u_gt_neg_items,
                            pd_rank=item_rank
                        )

                        iteration_results[iteration] = mt_score

                    user_results[i] = iteration_results
                    reranked_results_per_method[j][i][user_idx] = iteration_results
                users_single_metric_result = user_results[i]
                # for iteration, single_metric in users_single_metric_result.items():
                #     # all iterations result for one user
                avg_for_one_user = sum(users_single_metric_result.values())/len(users_single_metric_result)
                avg_results[i] = avg_for_one_user
                reranked_results_avg_per_user[j][i][user_idx] = avg_results[i]
                    

    for reranker_id in range(len(rerankers)):
        result = []
        for i, mt in enumerate(metrics):

            user_avg_over_iters_single_metric = reranked_results_avg_per_user[reranker_id][i]
            sum_values_all_users = 0
            total_users = 0
            if not isinstance(mt, MAP) and not isinstance(mt, AUC):
                for u_id, avg_for_one_user in user_avg_over_iters_single_metric.items():

                    sum_values_all_users += avg_for_one_user
                    total_users += 1

                if total_users > 0:
                  
                    result = sum_values_all_users / total_users

                    reranked_avg_results[reranker_id][i] = result
                else:
                    reranked_avg_results[reranker_id][i] = -1
                

    return reranked_avg_results, reranked_results_per_method

def preprocess_data_for_Fragmentation(user_idx, test_set,  train_set, model, reranker, metrics, item_indices):
    '''
    Preprocess data for evaluating the "Fragmentation" metric.

    This function prepares sampled ranked lists of items for other users to evaluate 
    metrics involving fragmentation. It ensures the sampled data excludes the current user 
    and adheres to the constraints defined by the metric, such as the number of samples (`n_samples`) 
    and the top-k items (`k`).

    Parameters:
    - `user_idx`: The index of the current user being evaluated.
    - `test_set`: The test dataset containing user-item interactions.
    - `train_set`: The training dataset containing user-item interactions.
    - `model`: The recommender model used to generate ranked lists.
    - `reranker`: The re-ranking object responsible for adjusting the ranked lists.
    - `metrics`: A list of evaluation metrics, including the "Fragmentation" metric.
    - `item_indices`: A list of item indices to consider during ranking and re-ranking.

    Returns:
    - `pd_other_users`: A list of lists where each sub-list corresponds to a metric. 
        For metrics other than 'Fragmentation,' an empty list is used.

    '''
    pd_other_users = [] 
    max_k = max(m.k for m in metrics)

    for i, mt in enumerate(metrics):
        if "Fragmentation" in mt.name:
            if len(model.ranked_items) > mt.n_samples:
                other_users = [key for key,
                               value in model.ranked_items.items()]

                # Exclude the current user (user_idx) from the candidate list
                other_users.remove(user_idx)

            else:
                test_user_indices = set(test_set.uir_tuple[0])
                other_users = list(test_user_indices)
                other_users.remove(user_idx)  # Exclude the current user

            # Sample from other users
            sampled_users = np.random.choice(
                other_users, size=mt.n_samples, replace=False)

            # Process samples based on the value of mt.k
            sample_rank = []

            # Separate cached and uncached samples
            for x in sampled_users:


                model_ranked_items, model_ranked_scores = cache_rankings(
        model,   user_idx = x, item_indices=item_indices, k=-1)
                
        
                reranked_items_x  = cache_dynamic_rerankings(reranker, x, train_set, initial_item_rank = model_ranked_items, recommendation_list = model_ranked_items, prediction_scores= None) 
                random_iter = random.choice(list(reranked_items_x.keys()))
                selected_random_iter_recom = reranked_items_x[random_iter]
                
                if len(selected_random_iter_recom) >= mt.k and mt.k > 0:
                    sample_rank.append(selected_random_iter_recom[:mt.k])
                else:
                    sample_rank.append(selected_random_iter_recom)

            pd_other_users.append(sample_rank)
        else:
            pd_other_users.append([])
    return pd_other_users




def diversity_eval_on_dyn_rerankers(
        model,
        metrics,
        rerankers,
        train_set,
        test_set,
        val_set=None,
        rating_threshold=1.0,
        exclude_unknowns=True,
        verbose=False
):
    """ Evaluate recommendations generated by the dynamic re-ranking process using diversity metrics.
    Parameters
    ----------
    model: :obj:`cornac.models.Recommender`, required
        Recommender model to be evaluated.

    metrics: obj:`iterable`, required
        List of diversity metrics. obj:`cornac.metrics.DiversityMetric`.
    
    rerankers: A list of dynamic rerankers to apply on the model's output.

    train_set: obj:`cornac.data.Dataset`, required
        Training dataset used to exclude training interactions from evaluation.

    test_set: obj:`cornac.data.Dataset`, required
        Test dataset for evaluation.

    val_set: :obj:`cornac.data.Dataset`, optional, default: None
       Validation dataset. 

    exclude_unknowns: bool, optional, default: True
        Ignore unknown users and items during evaluation.

    rating_threshold: float, optional, default: 1.0
        The threshold to convert ratings into positive or negative feedback.


    verbose: bool, optional, default: False
        Output evaluation progress.


    Returns
    -------
    avg_results : list
        Average metric scores across all users for each metric.

    user_results : list of dict
        Per-user metric scores for each metric, with results grouped by users.

    """
    if len(metrics) == 0:
        return [], []

    reranked_results_per_method = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]
    
    reranked_results_avg_per_user = [
        [{} for _ in enumerate(metrics)] for _ in range(len(rerankers))]
    
    reranked_avg_results = [
        [0 for _ in enumerate(metrics)] for _ in range(len(rerankers))]

        # Initialize cache if not already present
    if not hasattr(model, 'ranked_items'):
        model.ranked_items = {}
    if not hasattr(model, 'item_scores'):
        model.item_scores = {}


    test_mat = test_set.csr_matrix
    train_mat = train_set.csr_matrix
    val_mat = None if val_set is None else val_set.csr_matrix

    def pos_items(csr_row):
        return [
            item_idx
            for (item_idx, rating) in zip(csr_row.indices, csr_row.data)
            if rating >= rating_threshold
        ]

    user_history_dict = OrderedDict()

    test_user_indices = set(test_set.uir_tuple[0])
    for user_idx in test_user_indices:
        pos_item_idx = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )
        user_history_dict[user_idx] = pos_item_idx
    
    # check if metrics contain Binomial
    globalProbs = []

    for i, mt in enumerate(metrics):
        if "Binomial" in mt.name:
            global_prob = mt.globalFeatureProbs(user_history_dict)
            globalProbs.append(global_prob)
        else:
            globalProbs.append([])

    for user_idx in tqdm(
            test_user_indices, desc="Diversity evaluation on Dynamic rerankers", disable=not verbose, miniters=100
    ):
        test_pos_items = pos_items(test_mat.getrow(
            user_idx))
        if len(test_pos_items) == 0:
            continue

        # binary mask for ground-truth positive items
        u_gt_pos_mask = np.zeros(test_set.num_items, dtype="int")
        u_gt_pos_mask[test_pos_items] = 1

        val_pos_items = [] if val_mat is None else pos_items(
            val_mat.getrow(user_idx))
        
        train_pos_items = (
            pos_items(train_mat.getrow(user_idx))
            if user_idx < train_mat.shape[0]
            else []
        )

        u_gt_neg_mask = np.ones(test_set.num_items, dtype="int")
        u_gt_neg_mask[test_pos_items + val_pos_items + train_pos_items] = 0

        if exclude_unknowns:
            u_gt_pos_mask = u_gt_pos_mask[: train_set.num_items]
            u_gt_neg_mask = u_gt_neg_mask[: train_set.num_items]
        
        item_indices = np.nonzero(u_gt_pos_mask + u_gt_neg_mask)[0]
        # item_rank, item_scores = cache_rankings(
        # model, user_idx, item_indices, k = -1)
        item_rank, item_scores = cache_rankings(
        model,   user_idx=user_idx, item_indices=item_indices,  k=-1)
        
        # item_idx_to_score = {item: score for item,
        #                  score in zip(item_indices, item_scores)}
        # Retrieve the scores corresponding to ranked_items
        # ranked_scores = [item_idx_to_score[item] for item in item_rank]
        ranked_items = list(item_rank)
    

        pool_ids = np.arange(test_set.num_items)
        
        u_gt_rating = np.zeros(test_set.num_items)
        gd_item_idx = test_mat.getrow(user_idx).indices
        gd_item_rating = test_mat.getrow(user_idx).data
        u_gt_rating[gd_item_idx] = gd_item_rating
        # interacted and positive rating in training set
        user_history = user_history_dict.get(user_idx, [])
        
        for j in range(len(rerankers)):
            reranker = rerankers[j]
            result_all_iterations  = cache_dynamic_rerankings(reranker, user_idx, train_set,  initial_item_rank = ranked_items, recommendation_list = ranked_items, prediction_scores= None) 
            pd_other_users = preprocess_data_for_Fragmentation(
                user_idx, test_set,  train_set, model, reranker, metrics, item_indices)

            if len(result_all_iterations) == 0:
                continue
            
            avg_results = [{} for _ in enumerate(metrics)]
            user_results = [{} for _ in enumerate(metrics)]

            for i, mt in enumerate(metrics):

                iteration_results = {}
                for iteration, item_rank in result_all_iterations.items():
                    mt_score = mt.compute(
                        pd_rank=item_rank,
                        # pd_scores=item_scores,
                        rating_threshold=rating_threshold,
                        gt_ratings=u_gt_rating,
                        globalProb=globalProbs[i],
                        user_history=user_history,
                        pool=pool_ids,
                        pd_other_users=pd_other_users[i]
                    )

                    if mt_score is None:
                        pass
                    else:
                        iteration_results[iteration] = mt_score
                if len(iteration_results)>0:
                    user_results[i] = iteration_results
                    reranked_results_per_method[j][i][user_idx] = iteration_results
                    users_single_metric_result = user_results[i]
                    # for iteration, single_metric in users_single_metric_result.items():
                    #     # all iterations result for one user
                    avg_for_one_user = sum(users_single_metric_result.values())/len(users_single_metric_result)
                    avg_results[i] = avg_for_one_user
                    reranked_results_avg_per_user[j][i][user_idx] = avg_results[i]

    for reranker_id in range(len(rerankers)):
        for i, mt in enumerate(metrics):
            user_avg_over_iters_single_metric = reranked_results_avg_per_user[reranker_id][i]
            sum_values_all_users = 0
            total_users = 0
            # print(f"{mt.name},value is {user_avg_over_iters_single_metric}")
           
            for u_id, avg_for_one_user in user_avg_over_iters_single_metric.items():
                sum_values_all_users += avg_for_one_user
                total_users += 1

            if total_users > 0:
                result = sum_values_all_users / total_users

                reranked_avg_results[reranker_id][i] = result
            else:
                reranked_avg_results[reranker_id][i] = -1

    return reranked_avg_results, reranked_results_per_method


class DynamicReRankEval():
    """ Dynamic re-ranking evaluation framework.
    """

    def __init__(
            self, BaseEvaluator):
        """
        Initialize the DynamicReRankEval framework.

        Parameters
        ----------
        BaseEvaluator : object
            Base evaluator object with the necessary datasets and configuration.
            E.g., a "cornac.eval_methods.RatioSplit" object.

        """
        self.BaseEvaluator = BaseEvaluator

    # def _eval(self, initial_recommender_config, test_set, val_set, rerankers, initial_recommendation, initial_item_rank):
    def _eval(self, model, test_set, val_set,  rerankers,  rating_metrics,
        ranking_metrics,
        diversity_metrics):
        """
        Internal method to evaluate ranking and diversity metrics for dynamic re-ranked recommendations.

        Parameters
        ----------
        model : Recommender model.

        test_set : cornac.data.Dataset
            Test dataset for evaluation.

        val_set : cornac.data.Dataset, optional
            Validation dataset.

        rerankers : list of cornac.rerankers.DynamicReRanker
            List of dynamic re-rankers to evaluate.

        Metrics

        Returns
        -------
        Result : cornac.experiment.Result
            Evaluation results containing metric averages and per-user scores.
        """

        metric_avg_results = OrderedDict()
        metric_user_results = OrderedDict()

        reranked_avg_results, reranked_results_per_method = ranking_eval_on_dyn_rerankers(
            model=model,
            metrics= ranking_metrics,
            rerankers=rerankers,
            train_set=self.BaseEvaluator.train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=self.BaseEvaluator.rating_threshold,
            exclude_unknowns=self.BaseEvaluator.exclude_unknowns,
            verbose=self.BaseEvaluator.verbose
        )
        for j in range(len(rerankers)):
            for i, mt in enumerate(ranking_metrics):
                # new_name = model.name+"_"+rerankers[j].name + "_" + mt.name
                new_name = rerankers[j].name + "_" + mt.name
                metric_avg_results[new_name] = reranked_avg_results[j][i]
                metric_user_results[new_name] = reranked_results_per_method[j][i]

        reranked_avg_results, reranked_results_per_method = diversity_eval_on_dyn_rerankers(
            model=model,
            metrics= diversity_metrics,
            rerankers=rerankers,
            train_set=self.BaseEvaluator.train_set,
            test_set=test_set,
            val_set=val_set,
            rating_threshold=self.BaseEvaluator.rating_threshold,
            exclude_unknowns=self.BaseEvaluator.exclude_unknowns,
            verbose=self.BaseEvaluator.verbose
        )

        for j in range(len(rerankers)):
            for i, mt in enumerate(diversity_metrics):
                # new_name =  model.name+"_"+rerankers[j].name + "_"+mt.name
                new_name = rerankers[j].name + "_" + mt.name
                metric_avg_results[new_name] = reranked_avg_results[j][i]
                metric_user_results[new_name] = reranked_results_per_method[j][i]

        avg_reranking_times = {}
        for reranker in rerankers:
            if hasattr(reranker, 'cumulative_time') and reranker.user_count > 0:
                avg_time = reranker.cumulative_time / reranker.user_count
                avg_reranking_times[reranker.name] = avg_time
                print(
                    f"Reranker: {reranker.name}, Avg. Reranking Time: {avg_time:.4f} seconds")
            else:
                avg_reranking_times[reranker.name] = None
        
        print(f"model:{model.name}, dynamic metric_avg_results:{metric_avg_results}")
        return Result(model.name, metric_avg_results, metric_user_results)
        # return Result(initial_recommender_config, metric_avg_results, metric_user_results,
        #               user_info=user_info, model_parameter=model_parameter)

    # def evaluate(self, initial_recommender_config, metrics,  dynamic_rerankers, initial_recommendation, initial_item_rank, show_validation=True):
    def evaluate(self, model, metrics, user_based, rerankers, show_validation=True):
        """Evaluate dynamic re-rankers on provided metrics.

        Parameters
        ----------
        model : Recommender model.

        metrics: obj:`iterable`
            List of ranking or diversity metrics.


        show_validation: bool, optional, default: True
            Whether to show the results on validation set (if exists).


        rerankers : list of cornac.rerankers.DynamicReRanker
            List of dynamic re-rankers to evaluate.

        show_validation : bool, optional, default=True
            Whether to include validation results in the output.


        Returns
        -------
        test_result : cornac.experiment.Result
            Evaluation results for the test dataset.

        val_result : cornac.experiment.Result, optional
            Evaluation results for the validation dataset, if applicable. Else, None.
        """
        if self.BaseEvaluator.train_set is None:
            raise ValueError("train_set is required but None!")
        if self.BaseEvaluator.test_set is None:
            raise ValueError("test_set is required but None!")
        if rerankers is None:
            raise ValueError("rerankers is required but None!")
        rating_metrics, ranking_metrics,diversity_metrics = self.BaseEvaluator.organize_metrics(metrics)
        ##############
        # Dynamic Re-rank #
        ##############
        if self.BaseEvaluator.verbose:
            print("\n[{}] Dynamic Re-ranking Evaluation started!".format(model.name))
        start = time.time()

        ##############
        # EVALUATION #
        ##############
        if self.BaseEvaluator.verbose:
            print("\nDynamic re-ranking Evaluation started!")

        start = time.time()
        test_result = self._eval(
            model=model,
            test_set=self.BaseEvaluator.test_set,
            val_set=self.BaseEvaluator.val_set,
            rerankers=rerankers,
            rating_metrics = rating_metrics ,
        ranking_metrics = ranking_metrics,
        diversity_metrics = diversity_metrics
        )
        test_time = time.time() - start
        test_result.metric_avg_results["Dynamic Re-Rank Time(s)"] = test_time
        # Optional: save  test result.
        # save_path = initial_recommender_config+"_trained/"+"dynamic_rerankers_exp"
        # test_result.save(save_path)

        val_result = None
        if show_validation and self.BaseEvaluator.val_set is not None:
            start = time.time()
            val_result = self._eval(
                model=model, test_set=self.BaseEvaluator.val_set, val_set=None, rerankers=rerankers,   rating_metrics = rating_metrics ,
        ranking_metrics = ranking_metrics,
        diversity_metrics = diversity_metrics
            )
            val_time = time.time() - start
            val_result.metric_avg_results["Dynamic Re-Rank Time(s)"] = val_time
        return test_result, val_result
